{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99613700",
   "metadata": {},
   "source": [
    "# 9. Evaluate the Hybrid Restoration Pipeline\n",
    "\n",
    "This notebook evaluates and compares the performance of three different image restoration methods:\n",
    "1.  **Real-ESRGAN Only**: The baseline model for detail enhancement.\n",
    "2.  **U-Net Only**: The PyTorch model trained for color and light correction.\n",
    "3.  **Hybrid Pipeline (U-Net + Real-ESRGAN)**: The 2-step pipeline that first corrects color/light with the U-Net and then enhances details with Real-ESRGAN.\n",
    "\n",
    "We will process a set of test images and compare the outputs both visually and quantitatively using PSNR and SSIM metrics against the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cd32e1",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports\n",
    "\n",
    "Import all necessary libraries, define file paths, and set up the device (GPU/CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64f02ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "U-Net model found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure the project root is in the Python path\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..')))\n",
    "\n",
    "from src.dl.hybrid_pipeline import HybridPipeline\n",
    "from src.dl.realesrgan_wrapper import RealESRGANWrapper\n",
    "from src.dl.pytorch_unet import UNet\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- Configuration ---\n",
    "UNET_MODEL_PATH = \"../outputs/models/unet/best_unet_model.pth\"\n",
    "REALESRGAN_MODEL_STR = \"x4\"\n",
    "TEST_IMAGE_DIR = \"../data/raw/AI_for_Art_Restoration_2/paired_dataset_art/damaged\"\n",
    "GROUND_TRUTH_DIR = \"../data/raw/AI_for_Art_Restoration_2/paired_dataset_art/undamaged\"\n",
    "OUTPUT_DIR = \"../outputs/evaluation_hybrid\"\n",
    "\n",
    "# Select a few images for testing\n",
    "TEST_IMAGES = [\"2.jpg\", \"5.jpg\", \"10.jpg\", \"15.jpg\"]\n",
    "\n",
    "# --- Setup ---\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check if U-Net model exists\n",
    "if not os.path.exists(UNET_MODEL_PATH):\n",
    "    print(f\"ERROR: U-Net model not found at {UNET_MODEL_PATH}\")\n",
    "    print(\"Please train the U-Net model first by running 'src/training/train_unet.py'\")\n",
    "else:\n",
    "    print(\"U-Net model found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d109a",
   "metadata": {},
   "source": [
    "### 2. Load Models\n",
    "\n",
    "Instantiate all three models:\n",
    "- The standalone `RealESRGANWrapper`.\n",
    "- The standalone `UNet`.\n",
    "- The combined `HybridPipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebecc8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U-Net model loaded successfully.\n",
      "Loaded Real-ESRGAN: RealESRGAN_x4plus | scale=4 | device=cuda\n",
      "Real-ESRGAN model loaded successfully.\n",
      "HybridPipeline using device: cuda\n",
      "Loading U-Net model from ../outputs/models/unet/best_unet_model.pth...\n",
      "U-Net model loaded successfully.\n",
      "Loading Real-ESRGAN model 'x4'...\n",
      "Loaded Real-ESRGAN: RealESRGAN_x4plus | scale=4 | device=cuda\n",
      "Real-ESRGAN model loaded successfully.\n",
      "Hybrid pipeline loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Load U-Net Model ---\n",
    "unet_model = UNet(n_channels=3, n_classes=3)\n",
    "if os.path.exists(UNET_MODEL_PATH):\n",
    "    unet_model.load_state_dict(torch.load(UNET_MODEL_PATH, map_location=device))\n",
    "    unet_model.to(device)\n",
    "    unet_model.eval()\n",
    "    print(\"U-Net model loaded successfully.\")\n",
    "\n",
    "# --- Load Real-ESRGAN Model ---\n",
    "realesrgan_model = RealESRGANWrapper(model_str=REALESRGAN_MODEL_STR)\n",
    "print(\"Real-ESRGAN model loaded successfully.\")\n",
    "\n",
    "# --- Load Hybrid Pipeline ---\n",
    "hybrid_pipeline = None\n",
    "if os.path.exists(UNET_MODEL_PATH):\n",
    "    hybrid_pipeline = HybridPipeline(UNET_MODEL_PATH, REALESRGAN_MODEL_STR, device)\n",
    "    print(\"Hybrid pipeline loaded successfully.\")\n",
    "\n",
    "# Transformation for U-Net input\n",
    "unet_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79332ee0",
   "metadata": {},
   "source": [
    "### 3. Define Helper and Metric Functions\n",
    "\n",
    "Create helper functions to:\n",
    "- Run each restoration model on an image.\n",
    "- Calculate PSNR and SSIM between two images.\n",
    "- Display the results in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef2c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_realesrgan_only(img):\n",
    "    \"\"\"Restores an image using only Real-ESRGAN.\"\"\"\n",
    "    # img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    restored_img, _ = realesrgan_model.restore(img)\n",
    "    print(type(restored_img))\n",
    "    print(restored_img)\n",
    "    return cv2.cvtColor(restored_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def restore_unet_only(img):\n",
    "    \"\"\"Restores an image using only the U-Net model.\"\"\"\n",
    "    if not os.path.exists(UNET_MODEL_PATH): return None\n",
    "    \n",
    "    # img = Image.open(image_path).convert(\"RGB\")\n",
    "    with torch.no_grad():\n",
    "        input_tensor = unet_transform(img).unsqueeze(0).to(device)\n",
    "        output_tensor = unet_model(input_tensor)\n",
    "    \n",
    "    # Convert tensor to PIL Image\n",
    "    output_img = transforms.ToPILImage()(output_tensor.squeeze(0).cpu())\n",
    "    return cv2.cvtColor(output_img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "def restore_hybrid(img):\n",
    "    \"\"\"Restores an image using the full hybrid pipeline.\"\"\"\n",
    "    if hybrid_pipeline is None: return None\n",
    "\n",
    "    # The hybrid pipeline's restore_image method saves to a file, \n",
    "    # so we'll adapt it to return the image instead for this notebook.\n",
    "    # img = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Step 1: U-Net\n",
    "    with torch.no_grad():\n",
    "        input_tensor = unet_transform(img).unsqueeze(0).to(device)\n",
    "        unet_output_tensor = hybrid_pipeline.unet(input_tensor)\n",
    "        unet_output_img_rgb = hybrid_pipeline.tensor_to_cv2(unet_output_tensor)\n",
    "        \n",
    "    # Step 2: Real-ESRGAN\n",
    "    unet_output_img_bgr = cv2.cvtColor(unet_output_img_rgb, cv2.COLOR_RGB2BGR)\n",
    "    final_output, _ = hybrid_pipeline.realesrgan.restore(unet_output_img_bgr)\n",
    "    return cv2.cvtColor(final_output, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def calculate_metrics(img1, img2):\n",
    "    \"\"\"Calculates PSNR and SSIM between two images.\"\"\"\n",
    "    img1_np = np.array(img1)\n",
    "    img2_np = np.array(img2)\n",
    "    \n",
    "    # Ensure images are the same size\n",
    "    h, w, _ = img1_np.shape\n",
    "    img2_np = cv2.resize(img2_np, (w, h))\n",
    "    \n",
    "    psnr_val = psnr(img1_np, img2_np, data_range=255)\n",
    "    ssim_val = ssim(img1_np, img2_np, channel_axis=2, data_range=255)\n",
    "    return psnr_val, ssim_val\n",
    "\n",
    "def plot_results(images, titles, figsize=(20, 10)):\n",
    "    \"\"\"Plots a list of images with their titles.\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, (img, title) in enumerate(zip(images, titles)):\n",
    "        plt.subplot(1, len(images), i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca38fe",
   "metadata": {},
   "source": [
    "### 4. Run Evaluation Loop\n",
    "\n",
    "Iterate through the test images, apply each restoration method, calculate metrics, and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7831cead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing: 2.jpg ---\n",
      "Skipping 2.jpg, file not found.\n",
      "--- Processing: 5.jpg ---\n",
      "<class 'PIL.Image.Image'>\n",
      "<class 'PIL.Image.Image'>\n",
      "<class 'tuple'>\n",
      "(array([[[254, 254, 254],\n",
      "        [246, 248, 248],\n",
      "        [248, 248, 249],\n",
      "        ...,\n",
      "        [247, 248, 247],\n",
      "        [247, 247, 246],\n",
      "        [247, 247, 245]],\n",
      "\n",
      "       [[252, 252, 252],\n",
      "        [113, 123, 146],\n",
      "        [101, 111, 139],\n",
      "        ...,\n",
      "        [126, 142, 152],\n",
      "        [129, 147, 158],\n",
      "        [125, 143, 159]],\n",
      "\n",
      "       [[253, 253, 252],\n",
      "        [111, 129, 158],\n",
      "        [109, 126, 158],\n",
      "        ...,\n",
      "        [130, 151, 163],\n",
      "        [130, 149, 163],\n",
      "        [128, 149, 164]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[251, 251, 252],\n",
      "        [ 19,  35,  45],\n",
      "        [ 18,  34,  53],\n",
      "        ...,\n",
      "        [ 48,  64,  86],\n",
      "        [ 42,  57,  79],\n",
      "        [ 40,  56,  79]],\n",
      "\n",
      "       [[253, 253, 253],\n",
      "        [ 19,  36,  45],\n",
      "        [ 21,  38,  53],\n",
      "        ...,\n",
      "        [ 47,  63,  85],\n",
      "        [ 43,  56,  78],\n",
      "        [ 36,  52,  74]],\n",
      "\n",
      "       [[253, 253, 253],\n",
      "        [ 26,  36,  48],\n",
      "        [ 20,  33,  52],\n",
      "        ...,\n",
      "        [ 47,  62,  83],\n",
      "        [ 42,  57,  78],\n",
      "        [ 42,  57,  79]]], dtype=uint8), 'RGB')\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src is not a numerical tuple\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m damaged_img_np = np.array(damaged_img)\n\u001b[32m     20\u001b[39m damaged_image_bgr = cv2.cvtColor(damaged_img_np, cv2.COLOR_RGB2BGR)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m realesrgan_restored = \u001b[43mrestore_realesrgan_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdamaged_image_bgr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m unet_restored = restore_unet_only(damaged_image_bgr)\n\u001b[32m     24\u001b[39m hybrid_restored = restore_hybrid(damaged_image_bgr)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mrestore_realesrgan_only\u001b[39m\u001b[34m(img)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(restored_img))\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(restored_img)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrestored_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.9.0) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src is not a numerical tuple\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
     ]
    }
   ],
   "source": [
    "results_data = []\n",
    "\n",
    "for img_name in TEST_IMAGES:\n",
    "    print(f\"--- Processing: {img_name} ---\")\n",
    "    damaged_path = os.path.join(TEST_IMAGE_DIR, img_name)\n",
    "    truth_path = os.path.join(GROUND_TRUTH_DIR, img_name)\n",
    "\n",
    "    if not os.path.exists(damaged_path) or not os.path.exists(truth_path):\n",
    "        print(f\"Skipping {img_name}, file not found.\")\n",
    "        continue\n",
    "\n",
    "    # Load images\n",
    "    damaged_img = Image.open(damaged_path).convert(\"RGB\")\n",
    "    print(type(damaged_img))\n",
    "    ground_truth_img = Image.open(truth_path).convert(\"RGB\")\n",
    "    print(type(ground_truth_img))\n",
    "\n",
    "    # --- Run Restoration ---\n",
    "    damaged_img_np = np.array(damaged_img)\n",
    "    damaged_image_bgr = cv2.cvtColor(damaged_img_np, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    realesrgan_restored = restore_realesrgan_only(damaged_image_bgr)\n",
    "    unet_restored = restore_unet_only(damaged_image_bgr)\n",
    "    hybrid_restored = restore_hybrid(damaged_image_bgr)\n",
    "\n",
    "    if unet_restored is None or hybrid_restored is None:\n",
    "        print(\"Skipping evaluation because U-Net model is not trained.\")\n",
    "        break\n",
    "\n",
    "    # --- Calculate Metrics ---\n",
    "    psnr_damaged, ssim_damaged = calculate_metrics(ground_truth_img, damaged_img)\n",
    "    psnr_realesrgan, ssim_realesrgan = calculate_metrics(ground_truth_img, realesrgan_restored)\n",
    "    psnr_unet, ssim_unet = calculate_metrics(ground_truth_img, unet_restored)\n",
    "    psnr_hybrid, ssim_hybrid = calculate_metrics(ground_truth_img, hybrid_restored)\n",
    "\n",
    "    # --- Store Results ---\n",
    "    results_data.append({\n",
    "        \"Image\": img_name,\n",
    "        \"PSNR_Damaged\": psnr_damaged, \"SSIM_Damaged\": ssim_damaged,\n",
    "        \"PSNR_U-Net\": psnr_unet, \"SSIM_U-Net\": ssim_unet,\n",
    "        \"PSNR_RealESRGAN\": psnr_realesrgan, \"SSIM_RealESRGAN\": ssim_realesrgan,\n",
    "        \"PSNR_Hybrid\": psnr_hybrid, \"SSIM_Hybrid\": ssim_hybrid,\n",
    "    })\n",
    "\n",
    "    # --- Display Visuals ---\n",
    "    images_to_plot = [damaged_img, ground_truth_img, unet_restored, realesrgan_restored, hybrid_restored]\n",
    "    titles = [\n",
    "        f\"Damaged\\nPSNR: {psnr_damaged:.2f}, SSIM: {ssim_damaged:.3f}\",\n",
    "        \"Ground Truth\",\n",
    "        f\"U-Net Only\\nPSNR: {psnr_unet:.2f}, SSIM: {ssim_unet:.3f}\",\n",
    "        f\"Real-ESRGAN Only\\nPSNR: {psnr_realesrgan:.2f}, SSIM: {ssim_realesrgan:.3f}\",\n",
    "        f\"Hybrid\\nPSNR: {psnr_hybrid:.2f}, SSIM: {ssim_hybrid:.3f}\"\n",
    "    ]\n",
    "    plot_results(images_to_plot, titles)\n",
    "\n",
    "# --- Create and display summary DataFrame ---\n",
    "if results_data:\n",
    "    df_results = pd.DataFrame(results_data)\n",
    "    display(df_results)\n",
    "else:\n",
    "    print(\"\\nNo results to display. Please ensure the U-Net model is trained and paths are correct.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2685af",
   "metadata": {},
   "source": [
    "### 5. Conclusion\n",
    "\n",
    "The table and images above provide a comprehensive comparison.\n",
    "\n",
    "- **Damaged**: The baseline scores for the input images.\n",
    "- **U-Net Only**: Should show significant improvement in color and lighting, which will be reflected in a higher PSNR/SSIM compared to the damaged version. The image might still appear blurry.\n",
    "- **Real-ESRGAN Only**: Should produce a sharp image, but the colors might be incorrect or washed out, similar to the damaged input. The PSNR/SSIM might not be high if the colors are wrong.\n",
    "- **Hybrid**: This should ideally have the best of both worlds: corrected colors from the U-Net and enhanced details from Real-ESRGAN, leading to the highest PSNR and SSIM scores.\n",
    "\n",
    "Based on the results, we can determine the effectiveness of the hybrid approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
