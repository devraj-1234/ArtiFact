{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99613700",
   "metadata": {},
   "source": [
    "# 9. Evaluate the Hybrid Restoration Pipeline\n",
    "\n",
    "This notebook evaluates and compares the performance of three different image restoration methods:\n",
    "1.  **Real-ESRGAN Only**: The baseline model for detail enhancement.\n",
    "2.  **U-Net Only**: The PyTorch model trained for color and light correction.\n",
    "3.  **Hybrid Pipeline (U-Net + Real-ESRGAN)**: The 2-step pipeline that first corrects color/light with the U-Net and then enhances details with Real-ESRGAN.\n",
    "\n",
    "We will process a set of test images and compare the outputs both visually and quantitatively using PSNR and SSIM metrics against the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cd32e1",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports\n",
    "\n",
    "Import all necessary libraries, define file paths, and set up the device (GPU/CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d64f02ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "U-Net model found.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make sure the project root is in the Python path\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), '..')))\n",
    "\n",
    "from src.dl.hybrid_pipeline import HybridPipeline\n",
    "from src.dl.realesrgan_wrapper import RealESRGANWrapper\n",
    "from src.dl.pytorch_unet import UNet\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- Configuration ---\n",
    "UNET_MODEL_PATH = \"../outputs/models/unet/best_unet_resnet34_perceptual.pth\"\n",
    "REALESRGAN_MODEL_STR = \"x4\"\n",
    "TEST_IMAGE_DIR = \"../data/raw/AI_for_Art_Restoration_2/paired_dataset_art/damaged\"\n",
    "GROUND_TRUTH_DIR = \"../data/raw/AI_for_Art_Restoration_2/paired_dataset_art/undamaged\"\n",
    "OUTPUT_DIR = \"../outputs/evaluation_hybrid\"\n",
    "\n",
    "# Select a few images for testing\n",
    "TEST_IMAGES = [\"1.png\", \"2.png\", \"3.png\", \"4.jpg\", \"5.jpg\"]\n",
    "\n",
    "# --- Setup ---\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check if U-Net model exists\n",
    "if not os.path.exists(UNET_MODEL_PATH):\n",
    "    print(f\"ERROR: U-Net model not found at {UNET_MODEL_PATH}\")\n",
    "    print(\"Please train the U-Net model first by running 'src/training/train_unet.py'\")\n",
    "else:\n",
    "    print(\"U-Net model found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d109a",
   "metadata": {},
   "source": [
    "### 2. Load Models\n",
    "\n",
    "Instantiate all three models:\n",
    "- The standalone `RealESRGANWrapper`.\n",
    "- The standalone `UNet`.\n",
    "- The combined `HybridPipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebecc8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: segmentation-models-pytorch in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from segmentation-models-pytorch) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.19.3 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from segmentation-models-pytorch) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from segmentation-models-pytorch) (12.0.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from segmentation-models-pytorch) (0.6.2)\n",
      "Requirement already satisfied: timm>=0.9 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from segmentation-models-pytorch) (1.0.21)\n",
      "Requirement already satisfied: torch>=1.8 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from segmentation-models-pytorch) (2.1.2+cu118)\n",
      "Requirement already satisfied: torchvision>=0.9 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from segmentation-models-pytorch) (0.16.2+cu118)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from segmentation-models-pytorch) (4.67.1)\n",
      "Requirement already satisfied: filelock in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.3)\n",
      "Requirement already satisfied: requests in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.15.0)\n",
      "Requirement already satisfied: sympy in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from torch>=1.8->segmentation-models-pytorch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from torch>=1.8->segmentation-models-pytorch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n",
      "Requirement already satisfied: colorama in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from tqdm>=4.42.1->segmentation-models-pytorch) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\r&d project\\image_processing\\venv\\lib\\site-packages (from sympy->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "U-Net model loaded successfully.\n",
      "Loaded Real-ESRGAN: RealESRGAN_x4plus | scale=4 | device=cuda\n",
      "Real-ESRGAN model loaded successfully.\n",
      "HybridPipeline using device: cuda\n",
      "Loading U-Net model from ../outputs/models/unet/best_unet_resnet34_perceptual.pth...\n",
      "U-Net model loaded successfully.\n",
      "Loading Real-ESRGAN model 'x4'...\n",
      "Loaded Real-ESRGAN: RealESRGAN_x4plus | scale=4 | device=cuda\n",
      "Real-ESRGAN model loaded successfully.\n",
      "Loading GFPGAN for face enhancement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFPGAN model loaded successfully.\n",
      "Hybrid pipeline loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "%pip install segmentation-models-pytorch\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# --- Load U-Net Model ---\n",
    "# The model was trained using segmentation-models-pytorch with a resnet34 backbone\n",
    "unet_model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",\n",
    "    encoder_weights=None, # Not loading pretrained imagenet weights, will load our own\n",
    "    in_channels=3,\n",
    "    classes=3,\n",
    ")\n",
    "if os.path.exists(UNET_MODEL_PATH):\n",
    "    # The saved state dict is from the model itself, not a checkpoint dictionary\n",
    "    state_dict = torch.load(UNET_MODEL_PATH, map_location=device)\n",
    "    unet_model.load_state_dict(state_dict)\n",
    "    unet_model.to(device)\n",
    "    unet_model.eval()\n",
    "    print(\"U-Net model loaded successfully.\")\n",
    "\n",
    "# --- Load Real-ESRGAN Model ---\n",
    "realesrgan_model = RealESRGANWrapper(model_str=REALESRGAN_MODEL_STR)\n",
    "print(\"Real-ESRGAN model loaded successfully.\")\n",
    "\n",
    "# --- Load Hybrid Pipeline ---\n",
    "hybrid_pipeline = None\n",
    "if os.path.exists(UNET_MODEL_PATH):\n",
    "    hybrid_pipeline = HybridPipeline(UNET_MODEL_PATH, REALESRGAN_MODEL_STR, device)\n",
    "    print(\"Hybrid pipeline loaded successfully.\")\n",
    "\n",
    "# Transformation for U-Net input\n",
    "unet_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79332ee0",
   "metadata": {},
   "source": [
    "### 3. Define Helper and Metric Functions\n",
    "\n",
    "Create helper functions to:\n",
    "- Run each restoration model on an image.\n",
    "- Calculate PSNR and SSIM between two images.\n",
    "- Display the results in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eef2c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_realesrgan_only(img_bgr):\n",
    "    \"\"\"Restores an image using only Real-ESRGAN.\"\"\"\n",
    "    # img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    restored_img_bgr, _ = realesrgan_model.restore(img_bgr)\n",
    "    print(f\" Type of restored image under restore_realesrgan_only() is {type(restored_img_bgr)}\")\n",
    "    print(restored_img_bgr)\n",
    "    restored_img_bgr = restored_img_bgr[0]\n",
    "    return cv2.cvtColor(restored_img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def restore_unet_only(img_pil):\n",
    "    \"\"\"Restores an image using only the U-Net model.\"\"\"\n",
    "    if not os.path.exists(UNET_MODEL_PATH): return None\n",
    "    \n",
    "    # img = Image.open(image_path).convert(\"RGB\")\n",
    "    with torch.no_grad():\n",
    "        input_tensor = unet_transform(img_pil).unsqueeze(0).to(device)\n",
    "        output_tensor = unet_model(input_tensor)\n",
    "    \n",
    "    # Convert tensor to PIL Image\n",
    "    output_img = transforms.ToPILImage()(output_tensor.squeeze(0).cpu())\n",
    "    return np.array(output_img)\n",
    "\n",
    "def restore_hybrid(img_pil):\n",
    "    \"\"\"Restores an image using the full hybrid pipeline.\"\"\"\n",
    "    if hybrid_pipeline is None: return None\n",
    "\n",
    "    # The hybrid pipeline's restore_image method saves to a file, \n",
    "    # so we'll adapt it to return the image instead for this notebook.\n",
    "    # img = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Step 1: U-Net\n",
    "    with torch.no_grad():\n",
    "        input_tensor = unet_transform(img_pil).unsqueeze(0).to(device)\n",
    "        unet_output_tensor = hybrid_pipeline.unet(input_tensor)\n",
    "        unet_output_bgr = hybrid_pipeline.tensor_to_cv2(unet_output_tensor)\n",
    "        \n",
    "    # Step 2: Real-ESRGAN\n",
    "    # unet_output_img_bgr = cv2.cvtColor(unet_output_img_rgb, cv2.COLOR_RGB2BGR)\n",
    "    final_output_bgr, _ = hybrid_pipeline.realesrgan.restore(unet_output_bgr)\n",
    "    return cv2.cvtColor(final_output_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def calculate_metrics(img1, img2):\n",
    "    \"\"\"Calculates PSNR and SSIM between two images.\"\"\"\n",
    "    img1_np = np.array(img1)\n",
    "    img2_np = np.array(img2)\n",
    "    \n",
    "    # Ensure images are the same size\n",
    "    h, w, _ = img1_np.shape\n",
    "    img2_np = cv2.resize(img2_np, (w, h))\n",
    "    \n",
    "    psnr_val = psnr(img1_np, img2_np, data_range=255)\n",
    "    ssim_val = ssim(img1_np, img2_np, channel_axis=2, data_range=255)\n",
    "    return psnr_val, ssim_val\n",
    "\n",
    "def plot_results(images, titles, figsize=(20, 10)):\n",
    "    \"\"\"Plots a list of images with their titles.\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, (img, title) in enumerate(zip(images, titles)):\n",
    "        plt.subplot(1, len(images), i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca38fe",
   "metadata": {},
   "source": [
    "### 4. Run Evaluation Loop\n",
    "\n",
    "Iterate through the test images, apply each restoration method, calculate metrics, and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a29aef5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "986323456\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    940 MiB |    940 MiB |   1035 MiB |  96785 KiB |\n",
      "|       from large pool |    729 MiB |    729 MiB |    817 MiB |  89344 KiB |\n",
      "|       from small pool |    210 MiB |    210 MiB |    218 MiB |   7441 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    940 MiB |    940 MiB |   1035 MiB |  96785 KiB |\n",
      "|       from large pool |    729 MiB |    729 MiB |    817 MiB |  89344 KiB |\n",
      "|       from small pool |    210 MiB |    210 MiB |    218 MiB |   7441 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |    925 MiB |    925 MiB |   1018 MiB |  95530 KiB |\n",
      "|       from large pool |    714 MiB |    714 MiB |    801 MiB |  88128 KiB |\n",
      "|       from small pool |    210 MiB |    210 MiB |    217 MiB |   7402 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    962 MiB |    962 MiB |    962 MiB |      0 B   |\n",
      "|       from large pool |    748 MiB |    748 MiB |    748 MiB |      0 B   |\n",
      "|       from small pool |    214 MiB |    214 MiB |    214 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  21881 KiB |  66311 KiB | 745976 KiB | 724094 KiB |\n",
      "|       from large pool |  18688 KiB |  59904 KiB | 560768 KiB | 542080 KiB |\n",
      "|       from small pool |   3193 KiB |   7152 KiB | 185208 KiB | 182014 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    3218    |    3218    |    3496    |     278    |\n",
      "|       from large pool |     172    |     172    |     193    |      21    |\n",
      "|       from small pool |    3046    |    3046    |    3303    |     257    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    3218    |    3218    |    3496    |     278    |\n",
      "|       from large pool |     172    |     172    |     193    |      21    |\n",
      "|       from small pool |    3046    |    3046    |    3303    |     257    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     139    |     139    |     139    |       0    |\n",
      "|       from large pool |      32    |      32    |      32    |       0    |\n",
      "|       from small pool |     107    |     107    |     107    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      33    |      60    |     166    |     133    |\n",
      "|       from large pool |       8    |      11    |      42    |      34    |\n",
      "|       from small pool |      25    |      52    |     124    |      99    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "None\n",
      "986323456\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_summary())\n",
    "print(torch.cuda.empty_cache())\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7831cead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing: 1.png ---\n",
      "<class 'PIL.Image.Image'>\n",
      "<class 'PIL.Image.Image'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m damaged_img_np = np.array(damaged_img_pil)\n\u001b[32m     21\u001b[39m damaged_image_bgr = cv2.cvtColor(damaged_img_np, cv2.COLOR_RGB2BGR)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m realesrgan_restored = \u001b[43mrestore_realesrgan_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdamaged_image_bgr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m unet_restored = restore_unet_only(damaged_img_pil)\n\u001b[32m     25\u001b[39m hybrid_restored = restore_hybrid(damaged_img_pil)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mrestore_realesrgan_only\u001b[39m\u001b[34m(img_bgr)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Restores an image using only Real-ESRGAN.\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m restored_img_bgr, _ = \u001b[43mrealesrgan_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_bgr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Type of restored image under restore_realesrgan_only() is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(restored_img_bgr)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(restored_img_bgr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\r&d project\\image_processing\\src\\dl\\realesrgan_wrapper.py:133\u001b[39m, in \u001b[36mRealESRGANWrapper.restore\u001b[39m\u001b[34m(self, image, outscale)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrestore\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np.ndarray, outscale: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m1.0\u001b[39m) -> Tuple[np.ndarray, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[32m    123\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03m    Enhance an image.\u001b[39;00m\n\u001b[32m    125\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    131\u001b[39m \u001b[33;03m        A tuple containing the restored image and None.\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     restored_image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrestorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m restored_image, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\r&d project\\image_processing\\src\\dl\\realesrgan_wrapper.py:200\u001b[39m, in \u001b[36mRealESRGANRestorer.restore\u001b[39m\u001b[34m(self, image, outscale)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, np.ndarray):\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInput image must be a non-empty numpy array (BGR, uint8)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m output, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupsampler\u001b[49m\u001b[43m.\u001b[49m\u001b[43menhance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output, _\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\realesrgan\\utils.py:223\u001b[39m, in \u001b[36mRealESRGANer.enhance\u001b[39m\u001b[34m(self, img, outscale, alpha_upsampler)\u001b[39m\n\u001b[32m    221\u001b[39m     \u001b[38;5;28mself\u001b[39m.tile_process()\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    224\u001b[39m output_img = \u001b[38;5;28mself\u001b[39m.post_process()\n\u001b[32m    225\u001b[39m output_img = output_img.data.squeeze().float().cpu().clamp_(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m).numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\realesrgan\\utils.py:115\u001b[39m, in \u001b[36mRealESRGANer.process\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    114\u001b[39m     \u001b[38;5;66;03m# model inference\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28mself\u001b[39m.output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1518\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1522\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1523\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1525\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1526\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1529\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1530\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\basicsr\\archs\\rrdbnet_arch.py:113\u001b[39m, in \u001b[36mRRDBNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    111\u001b[39m     feat = x\n\u001b[32m    112\u001b[39m feat = \u001b[38;5;28mself\u001b[39m.conv_first(feat)\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m body_feat = \u001b[38;5;28mself\u001b[39m.conv_body(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    114\u001b[39m feat = feat + body_feat\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# upsample\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1518\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1522\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1523\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1525\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1526\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1529\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1530\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1518\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1522\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1523\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1525\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1526\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1529\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1530\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\basicsr\\archs\\rrdbnet_arch.py:59\u001b[39m, in \u001b[36mRRDB.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrdb1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.rdb2(out)\n\u001b[32m     61\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.rdb3(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1518\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1522\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1523\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1525\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1526\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1529\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1530\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\basicsr\\archs\\rrdbnet_arch.py:36\u001b[39m, in \u001b[36mResidualDenseBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     34\u001b[39m x2 = \u001b[38;5;28mself\u001b[39m.lrelu(\u001b[38;5;28mself\u001b[39m.conv2(torch.cat((x, x1), \u001b[32m1\u001b[39m)))\n\u001b[32m     35\u001b[39m x3 = \u001b[38;5;28mself\u001b[39m.lrelu(\u001b[38;5;28mself\u001b[39m.conv3(torch.cat((x, x1, x2), \u001b[32m1\u001b[39m)))\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m x4 = \u001b[38;5;28mself\u001b[39m.lrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx3\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     37\u001b[39m x5 = \u001b[38;5;28mself\u001b[39m.conv5(torch.cat((x, x1, x2, x3, x4), \u001b[32m1\u001b[39m))\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Empirically, we use 0.2 to scale the residual for better performance\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1518\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1522\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1523\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1525\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1526\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1529\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1530\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\R&D Project\\image_processing\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m'\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    453\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(F.pad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode),\n\u001b[32m    454\u001b[39m                     weight, bias, \u001b[38;5;28mself\u001b[39m.stride,\n\u001b[32m    455\u001b[39m                     _pair(\u001b[32m0\u001b[39m), \u001b[38;5;28mself\u001b[39m.dilation, \u001b[38;5;28mself\u001b[39m.groups)\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "results_data = []\n",
    "\n",
    "for img_name in TEST_IMAGES:\n",
    "    print(f\"--- Processing: {img_name} ---\")\n",
    "    torch.cuda.empty_cache()\n",
    "    damaged_path = os.path.join(TEST_IMAGE_DIR, img_name)\n",
    "    truth_path = os.path.join(GROUND_TRUTH_DIR, img_name)\n",
    "\n",
    "    if not os.path.exists(damaged_path) or not os.path.exists(truth_path):\n",
    "        print(f\"Skipping {img_name}, file not found.\")\n",
    "        continue\n",
    "\n",
    "    # Load images\n",
    "    damaged_img_pil = Image.open(damaged_path).convert(\"RGB\")\n",
    "    print(type(damaged_img_pil))\n",
    "    ground_truth_img = Image.open(truth_path).convert(\"RGB\")\n",
    "    print(type(ground_truth_img))\n",
    "\n",
    "    # --- Run Restoration ---\n",
    "    damaged_img_np = np.array(damaged_img_pil)\n",
    "    damaged_image_bgr = cv2.cvtColor(damaged_img_np, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    realesrgan_restored = restore_realesrgan_only(damaged_image_bgr)\n",
    "    unet_restored = restore_unet_only(damaged_img_pil)\n",
    "    hybrid_restored = restore_hybrid(damaged_img_pil)\n",
    "\n",
    "    if unet_restored is None or hybrid_restored is None:\n",
    "        print(\"Skipping evaluation because U-Net model is not trained.\")\n",
    "        break\n",
    "\n",
    "    # --- Calculate Metrics ---\n",
    "    psnr_damaged, ssim_damaged = calculate_metrics(ground_truth_img, damaged_img_pil)\n",
    "    psnr_realesrgan, ssim_realesrgan = calculate_metrics(ground_truth_img, realesrgan_restored)\n",
    "    psnr_unet, ssim_unet = calculate_metrics(ground_truth_img, unet_restored)\n",
    "    psnr_hybrid, ssim_hybrid = calculate_metrics(ground_truth_img, hybrid_restored)\n",
    "\n",
    "    # --- Store Results ---\n",
    "    results_data.append({\n",
    "        \"Image\": img_name,\n",
    "        \"PSNR_Damaged\": psnr_damaged, \"SSIM_Damaged\": ssim_damaged,\n",
    "        \"PSNR_U-Net\": psnr_unet, \"SSIM_U-Net\": ssim_unet,\n",
    "        \"PSNR_RealESRGAN\": psnr_realesrgan, \"SSIM_RealESRGAN\": ssim_realesrgan,\n",
    "        \"PSNR_Hybrid\": psnr_hybrid, \"SSIM_Hybrid\": ssim_hybrid,\n",
    "    })\n",
    "\n",
    "    # --- Display Visuals ---\n",
    "    images_to_plot = [damaged_img_pil, ground_truth_img, unet_restored, realesrgan_restored, hybrid_restored]\n",
    "    titles = [\n",
    "        f\"Damaged\\nPSNR: {psnr_damaged:.2f}, SSIM: {ssim_damaged:.3f}\",\n",
    "        \"Ground Truth\",\n",
    "        f\"U-Net Only\\nPSNR: {psnr_unet:.2f}, SSIM: {ssim_unet:.3f}\",\n",
    "        f\"Real-ESRGAN Only\\nPSNR: {psnr_realesrgan:.2f}, SSIM: {ssim_realesrgan:.3f}\",\n",
    "        f\"Hybrid\\nPSNR: {psnr_hybrid:.2f}, SSIM: {ssim_hybrid:.3f}\"\n",
    "    ]\n",
    "    plot_results(images_to_plot, titles)\n",
    "\n",
    "# --- Create and display summary DataFrame ---\n",
    "if results_data:\n",
    "    df_results = pd.DataFrame(results_data)\n",
    "    display(df_results)\n",
    "else:\n",
    "    print(\"\\nNo results to display. Please ensure the U-Net model is trained and paths are correct.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2685af",
   "metadata": {},
   "source": [
    "### 5. Conclusion\n",
    "\n",
    "The table and images above provide a comprehensive comparison.\n",
    "\n",
    "- **Damaged**: The baseline scores for the input images.\n",
    "- **U-Net Only**: Should show significant improvement in color and lighting, which will be reflected in a higher PSNR/SSIM compared to the damaged version. The image might still appear blurry.\n",
    "- **Real-ESRGAN Only**: Should produce a sharp image, but the colors might be incorrect or washed out, similar to the damaged input. The PSNR/SSIM might not be high if the colors are wrong.\n",
    "- **Hybrid**: This should ideally have the best of both worlds: corrected colors from the U-Net and enhanced details from Real-ESRGAN, leading to the highest PSNR and SSIM scores.\n",
    "\n",
    "Based on the results, we can determine the effectiveness of the hybrid approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
